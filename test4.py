import torch
import tempfile
import os
import streamlit as st

# X√¢y d·ª±ng vector database
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_huggingface import HuggingFacePipeline
from langchain_huggingface import HuggingFaceEmbeddings

# X√¢y d·ª±ng RAG Chain
from langchain.memory import ConversationBufferMemory
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import ConversationalRetrievalChain
from langchain_experimental.text_splitter import SemanticChunker

# X√¢y d·ª±ng Vector Database
from langchain_chroma import Chroma
from langchain import hub

# Kh·ªüi t·∫°o Session State
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None
if "model_loaded" not in st.session_state:
    st.session_state.model_loaded = False
if "embeddings" not in st.session_state:
    st.session_state.embeddings = None
if "llm" not in st.session_state:
    st.session_state.llm = None
if "messages" not in st.session_state:
    st.session_state.messages = []

# H√†m t·∫£i Embedding Model (cache model embeddings, tr√°nh vi·ªác t·∫£i l·∫°i nhi·ªÅu l·∫ßn)
@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(model_name="bkai-foundation-models/vietnamese-bi-encoder")

# H√†m t·∫£i LLM
@st.cache_resource
def load_llm():
    """
    T·∫£i v√† cache m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM) t·ª´ HuggingFace.
    Bao g·ªìm c∆° ch·∫ø fallback sang CPU n·∫øu GPU kh√¥ng kh·∫£ d·ª•ng.
    S·ª≠ d·ª•ng @st.cache_resource ƒë·ªÉ ch·ªâ t·∫£i m·ªôt l·∫ßn duy nh·∫•t.
    """
    MODEL_NAME = "lmsys/vicuna-7b-v1.5"
    st.info(f"ƒêang t·∫£i LLM: {MODEL_NAME}...")

    # Ki·ªÉm tra xem c√≥ GPU v√† CUDA c√≥ s·∫µn kh√¥ng
    if torch.cuda.is_available():
        st.info("Ph√°t hi·ªán GPU v√† CUDA. ƒêang t·∫£i m√¥ h√¨nh v·ªõi l∆∞·ª£ng t·ª≠ h√≥a 4-bit...")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            device_map="auto", # T·ª± ƒë·ªông ch·ªçn thi·∫øt b·ªã (GPU n·∫øu c√≥)
        )
    else:
        st.warning("Kh√¥ng t√¨m th·∫•y GPU ho·∫∑c CUDA Toolkit. M√¥ h√¨nh s·∫Ω ch·∫°y tr√™n CPU, c√≥ th·ªÉ r·∫•t ch·∫≠m v√† t·ªën RAM.")
        # Khi ch·∫°y tr√™n CPU, kh√¥ng s·ª≠ d·ª•ng BitsAndBytesConfig
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float32, # Th∆∞·ªùng d√πng float32 cho CPU
            low_cpu_mem_usage=True,
            device_map="cpu", # Bu·ªôc ch·∫°y tr√™n CPU
        )
        # L∆∞u √Ω: Khi ch·∫°y tr√™n CPU, m√¥ h√¨nh 7B c√≥ th·ªÉ t·ªën r·∫•t nhi·ªÅu RAM (kho·∫£ng 14GB cho float32)

    # T·∫£i tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        pad_token_id=tokenizer.eos_token_id,
        device=0 if torch.cuda.is_available() else -1
    )
    # Tr·∫£ v·ªÅ m√¥ h√¨nh pipeline
    return HuggingFacePipeline(pipeline=model_pipeline)


# H√†m x·ª≠ l√Ω PDF
def process_pdf(upload_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(upload_file.getvalue())
        tmp_file_path = tmp_file.name

    # T·∫°o ƒë·ªëi t∆∞·ª£ng PyPDFLoader ƒë·ªÉ t·∫£i t√†i li·ªáu PDF
    loader = PyPDFLoader(tmp_file_path)
    documents = loader.load()

    # X·ª≠ l√Ω t√†i li·ªáu PDF ƒë·ªÉ t·∫°o vector database
    semantic_splitter = SemanticChunker(
        embeddings=st.session_state.embeddings,
        buffer_size=1,
        breakpoint_threshold_type="percentile",
        breakpoint_threshold_amount=95,
        min_chunk_size=500,
        add_start_index=True
    )

    # Chia t√†i li·ªáu th√†nh c√°c ƒëo·∫°n nh·ªè h∆°n
    docs = semantic_splitter.split_documents(documents)
    vector_db = Chroma.from_documents(documents=docs, embedding=st.session_state.embeddings)
    retriever = vector_db.as_retriever()

    prompt = hub.pull("rlm/rag-prompt")# L·∫•y prompt t·ª´ LangChain Hub
    
    # H√†m format_docs kh√¥ng c·∫ßn thi·∫øt tr·ª±c ti·∫øp ·ªü ƒë√¢y n·ªØa v√¨ ConversationalRetrievalChain s·∫Ω t·ª± x·ª≠ l√Ω
    # def format_docs(docs):
    #   return "\n\n".join(doc.page_content for doc in docs)

    # T·∫°o ConversationalRetrievalChain c√≥ b·ªô nh·ªõ
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=st.session_state.llm,
        retriever=retriever,
        memory=ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        ),
        combine_docs_chain_kwargs={"prompt": prompt},
        chain_type="stuff"
    )

    os.unlink(tmp_file_path)
    return qa_chain, len(docs) #Tr·∫£ v·ªÅ qa_chain

# Giao di·ªán ng∆∞·ªùi d√πng Streamlit
st.set_page_config(page_title="PDF RAG Chatbot", page_icon=":robot:", layout="wide")

st.title("H·ªèi ƒë√°p PDF v·ªõi AI ü§ñ")

st.markdown("""
**·ª®ng d·ª•ng AI h·ªó tr·ª£ h·ªèi ƒë√°p tr·ª±c ti·∫øp v·ªõi n·ªôi dung t·∫£i l√™n t·ª´ file PDF b·∫±ng ti·∫øng Vi·ªát.**
**H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:**
1.  T·∫£i l√™n file PDF ch·ª©a n·ªôi dung b·∫°n mu·ªën h·ªèi ƒë√°p.
2.  Nh·∫•n n√∫t "X·ª≠ l√Ω PDF" v√† ch·ªù trong gi√¢y l√°t.
3.  Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v√†o √¥ b√™n d∆∞·ªõi v√† nh·∫•n Enter.
---
""")

# T·∫£i model
if not st.session_state.model_loaded:
    with st.spinner("ƒêang t·∫£i c√°c m√¥ h√¨nh AI, vui l√≤ng ch·ªù..."):
        st.session_state.embeddings = load_embeddings()
        st.session_state.llm = load_llm()
        st.session_state.model_loaded = True
        st.success("Model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!")

# T·∫£i file PDF
uploaded_file = st.file_uploader("T·∫£i l√™n file PDF", type=["pdf"])
if uploaded_file and st.button("X·ª≠ l√Ω PDF"):
    with st.spinner("ƒêang x·ª≠ l√Ω PDF..."):
        st.session_state.rag_chain, num_docs = process_pdf(uploaded_file)
        st.session_state.messages = []
        st.success(f"ƒê√£ x·ª≠ l√Ω {num_docs} ƒëo·∫°n t·ª´ t√†i li·ªáu PDF.")

# Giao di·ªán h·ªèi ƒë√°p
if st.session_state.rag_chain:
    # Hi·ªÉn th·ªã l·ªãch s·ª≠ n√≥i chuy·ªán
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Nh·∫≠p c√¢u h·ªèi
    question = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:")

    if question:
        # Th√™m c√¢u h·ªèi v√†o l·ªãch s·ª≠ chat
        st.session_state.messages.append({"role": "user", "content": question})

        with st.chat_message("user"):
            st.markdown(question)

        with st.spinner("ƒêang tr·∫£ l·ªùi..."):
            try:
                # G·ªçi rag_chain.invoke. Kh√¥ng c·∫ßn truy·ªÅn chat_history v√†o ƒë√¢y
                # v√¨ ConversationalRetrievalChain t·ª± qu·∫£n l√Ω b·ªô nh·ªõ
                output = st.session_state.rag_chain.invoke({"question": question})

                answer = output.get("answer", "Kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi.") # D√πng .get ƒë·ªÉ tr√°nh l·ªói n·∫øu key kh√¥ng t·ªìn t·∫°i

                # Th√™m c√¢u tr·∫£ l·ªùi c·ªßa AI v√†o l·ªãch s·ª≠ chat hi·ªÉn th·ªã
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
                # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi c·ªßa AI
                with st.chat_message("assistant"):
                    st.markdown(answer)

            except Exception as e:
                st.error(f"ƒê√£ x·∫£y ra l·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi: {e}")
                st.session_state.messages.append({"role": "assistant", "content": f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {e}"})
                with st.chat_message("assistant"):
                    st.markdown(f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {e}")
